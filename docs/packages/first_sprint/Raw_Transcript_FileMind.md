# Raw Transcript: FileMind Document Type Prediction API Design Session

Okay, so I'm going to be going through the Project 1 Prediction API full specification and assignment documents, building out both my vision for the underlying API—what we're predicting—as well as how I plan on addressing the various decisions that need to be made in each of these design documents. The whole point here is to essentially bootstrap ourselves into a situation where another model or agent, given this design spec and set of documents, could be given a task just like this one to go and realize the vision and perform the implementation following these design documents.

But for now, our deliverable is only the design documents because our professor has explicitly said that in this project, he's focused on getting us to work through a proper AI workflow. This involves doing detailed planning of something first and then leaving the implementation as a second task that is almost immediately achievable given these plans and design documents. It's almost a given or a straightforward process.

So, I'm going to start by going through the handout and project spec itself and how it relates to other projects that I've worked on. The whole idea with this prediction API is, as he said, that as students, we are likely working on other projects, and a prediction API is something that is nearly universal. It could be applied to pretty much any project that you would want.

I want to apply it to this FileMind project that I've been working on. It's a startup that's focused on data management and document control for enterprises with repeated project folder structures. Essentially, what it can do is index your full file system by creating a map of the underlying structure, pulling out metadata for every single file. Part of this metadata is the document type. For example, in construction, there are various types of documents, such as drawings, RFIs, specifications, reports, safety files, correspondence, contracts, submittals, quality estimates, cost reporting, etc.

The whole idea is that the people in these workflows, the construction project managers or whoever uses these file systems, use them to create more files. That's the whole point. There are really only three operations that can be made: a modify, a read, or a create. I suppose we might want to track deletes as well, but the main idea is we want to understand and predict how people use their file systems. For example, when creating a new contract, we always look at working files, letters of intent, and pull up these files as a reference to then go and create another file or do another one of these operations: update another file, create a new file, or read something else. Or delete.

The whole idea is that we have these organized file structures and metadata extracted from their files. That's the two services our project provides. We use it to dynamically place and name new files. But now, what we want to do is leverage that information to help predict the next operation that they'll take on their file system. The idea is that this prediction and this type of data could be collected by looking at the links that they navigate through their drive. So when you go into a different folder or open a specific file within Google Drive or SharePoint, we can track those actions. We can look at the sequence of these actions over time. Considering that they're likely a bursty data type, I'm not sure which type of prediction model is going to work best here.

But my idea is that for our universe of metadata that we extract—so the document type, among other qualities—and then the path down to this file, we can predict the next action. So, how would you navigate the folder structure to get here? From here, we would want to predict, okay, given that they have performed these actions recently—these previous actions, conditioning on those—what is the most likely folder or file action for them to take next? Creating a new file, updating something, or deleting.

The idea is that we can then provide suggestions for our users of which folders they might need to use to support their current work. Their current work is what our model is trying to identify. The underlying idea is, given what files they're using, what is our best guess at what type of workflow they're working under? And if we can answer that question, this prediction API could be very useful for augmenting how they do their job in terms of using these predictions with other agentic systems.

So now I'm going to go through the project spec template. First of all, who will use this and what decision does your prediction/design inform? The people who will use this API are the actual users—these construction project teams or project managers—but it will pass through our FileMind agent. In the sense that we make a prediction on which files this user might use next, we give that to our agent that has this understanding of their file system and use it to try to make predictions about what they will do next and reason about how they could be supported in this task. It informs job augmentation and, eventually, automation. Ideally, if you can do this task good enough, it's a very interesting step in automating workflows for businesses.

The potential targets are categorical in the sense that we are predicting one, what will be the metadata for the file that they will end up using, and where this would be expected to be in our project. I don't think we want to get all the way—because it's impossible to train a model that predicts all of those features, I think it makes the most sense to first just try to predict the document type of the user's action. So what is the document that they would do this operation on (create or modify) given their previous actions, metadata, and path in the folder structure?

It would be defined until they use another file, so that could be anywhere from a minute to a couple of hours. But it's a model that would be online, continuously online, in the sense that it can update its parameters and get feedback based on the decisions that the user actually makes. That's how you would gather data for this type of thing. It would make predictions that get better and better and better as it encounters more of your training data—so your specific use of your folder structure.

The possible features, as I just went over, are going to be the sequence of previous actions, metadata, and path in the folder structure. For simplicity and model tractability we are going to only focus on the action's corresponding document type metadata counts within the past hour.

So, the features, it says, are only information available at prediction time, and we want to avoid leakage. I think a good way to approach this problem would be to simply count the document types that they've interacted with over an hour-long window. For example, if they looked at two RFIs, one contract, and two drawings, then those would be the counts for our features. We are essentially looking at the occurrence of these events.

Because we are trying to predict a category here, we don't want to have a huge feature space, especially if we don't have enough observations across a high-dimensional space. That would result in sparse feature matrices, which leads to a real challenge in terms of meaningfully predicting the response without overfitting.

Essentially, our really baseline model is going to be this: based off the counts of the document types that the user has interacted with over the past period, which document type are they going to use next? That's all. That's all we're going to start with. We can have this broader vision of expanding our feature space and how we do the temporal estimation and the actual learning procedure at a future stage, because right now we're really just concerned about the prediction API.

To avoid leakage, we're not going to include any information about where they currently are in their directory, although that might be beneficial in the future. I'm just really concerned about not having enough data here. I feel like this would be a problem that would need a lot of data to be able to predict well. So, using a simple classifier here could be a way to do a proof of concept or an initial test to see how hard of a problem this really is.

Our baseline model that we could implement heuristically would be to predict whichever document type they used the most in the past hour, perhaps. That's equivalent to just predicting the mean, which is our baseline. But if we had a simple model, even something like a decision tree or logistic regression—because we're predicting categories here—a simple decision tree could, in theory, be trained to discover these relationships between certain file type usages and what they're going to use next. This will obviously be better than just predicting that they're going to use their most-used file, for example.

For our metric, I assume this is like our loss function. I think we can just use cross-entropy loss here because it's a classification task, and it would fit our model choice if we were to use logistic regression.

In terms of our service level agreement, our latency here could be a bit flexible. I don't think it would have to be that critical to have the latency be less than a couple hundred milliseconds. We would also want this to be cheap because we would be making this prediction every single time they use a file or interact with a file.

In terms of our API sketch, so the professor, when he's reviewing my work, can call it in his head, I think it's pretty simple. We would have a "predict" endpoint, which is running the model with the current feature set. And then maybe a GET method for health checking. I'm open to suggestions of what other endpoints might be necessary, as well as the request and response examples. But it would obviously be the feature set, and then the response would be the probabilities of each of our classes, which in this case are our document types.

So, for privacy, ethics, and reciprocity, for data inventory, purpose limitation, retention, and access, this would be really important data in that it's how people use their files. So, I would think it would be beneficial to not store their data long-term or be able to link it to individual users. Since we have these files already with metadata extracted that tell us our document type, we would want to essentially do a hash or a lookup of, given this file that they're currently on—so their current URL, building on the example that we've seen before in this course of Bitly—except here, we wouldn't want to look at the actual file content. So, this use of the link that they're currently on just informs, okay, given this huge list of all the files and their metadata that we have in a database, what is the document type for the one that we just accessed? And all the ones that we've accessed in our history that we're going to try to predict the next one from.

The value return would be the prediction of the next document type, and it would go back to our FileMind agent that sits on top of their file system. The architecture sketch will also have to include that, showing the flow and the happy pathway.

And then for risks and mitigations, I think the biggest risk is data quantity. I think training a model like this, it's quite important that we actually have representative training data. And our users, this is a variable that we don't currently capture, so we would have to build our training set as we go and do online learning. To reduce this risk, though, we could perhaps pool the data generated by different employees to train one global model.

Another risk, and an ethical one, is data leakage. So, if someone can see which document types have been used. But our mitigation is that we aren't storing individual user identifiable data, and it's not traceable back to the actual files that they were using. It's just the type of the document.

Another risk would be downtime or slow responses. This isn't like a mission-critical, life-or-death prediction API, so for that, we can just have fallbacks or the ability to move on to the next prediction if the previous one fails.

So, in experiments to validate the baseline, which would just be predicting the most-used one versus the model, this would just be to have someone monitor their usage of documents, perhaps over a week, give us this training data set, and then see if we can beat the baseline in terms of our cross-entropy loss.

So how will we measure SLA? I think we'll just have it built-in, in the sense that for every single prediction, we'd track the time and can display it on a dashboard to our user.

[Continues with PIA template discussion and additional details...]